{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8f4d11",
   "metadata": {},
   "source": [
    "## This code is a programming tutorial for the paper \" Interpretable Multi-Task XGBoost Model: An Enhanced Method to Estimate Reservoir Parameters from Logging Data\"\n",
    "\n",
    "## Author: Keran Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dd2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv(\"your_file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define label_processed\n",
    "def label_p(x1):\n",
    "    matrix = np.array([100,10,1,0.1])\n",
    "    bais_estimated = np.linalg.norm(matrix)\n",
    "    return 1/(1+np.exp(np.linalg.norm(x)+bais_estimated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a57011",
   "metadata": {},
   "source": [
    "### After you transfer the lithological labels (one-hot), the litho-classification becomes a regression task. We could use a XGBRegressor to complete the 1st-stage feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the processed label to train\n",
    "class first_stage_XGB(nn.Module):\n",
    "    def init(self, input_dim, output_dim, learning_rate, num_epochs):\n",
    "        super(XGBRegressor, self).init()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    # define foward\n",
    "    def forward(self, X, y):\n",
    "    self.model.fit(X, y)\n",
    "    return self.model.predict(X)\n",
    "\n",
    "    # model training\n",
    "    def train(self, x, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Forward pass\n",
    "            outputs = self.forward(x)\n",
    "            # Calculate loss\n",
    "            loss = self.criterion(outputs, y)\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # Print loss\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, self.num_epochs, loss.item()))\n",
    "        \n",
    "xgb_regressor = first_stage_XGB(input_dim=\"Number of logging data types\", output_dim=1, =\"The_learning_rate_you_want\", num_epochs=\"The_num_epochs_you_want\")\n",
    "xgb_regressor.train(\"Logging data\", \"Previous processed label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inverse_solution\n",
    "def inve_sol(x2):\n",
    "    return np.log(1/x2-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define task_calssification\n",
    "def task_class(x3):\n",
    "    x = inve_sol(x3)\n",
    "    return np.argmax(np.exp(x)/np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define choose\n",
    "def choose(x4):\n",
    "    p = np.array([0,0,0,1])\n",
    "    w = np.array([0,0,1,0])\n",
    "    m = np.array([0,1,0,0])\n",
    "    mm = np.array([1,0,0,0])\n",
    "    def ed(m1,m2):\n",
    "        return np.sqrt(np.sum((m1-m2)**2))\n",
    "    for i in range(4):\n",
    "        lith_class = np.array([ed(x4,p),ed(x4,w),ed(x4,m),ed(x4,mm)])\n",
    "    return  lith_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder\n",
    "def decoder(x5):\n",
    "    if task_class(x5)==0:\n",
    "        return choose(inve_sol(x5)/100-1)\n",
    "    if task_class(x5)==1:\n",
    "        return inve_sol(x5)/10-1\n",
    "    if task_class(x5)==2:\n",
    "        return inve_sol(x5)-1\n",
    "    if task_class(x5)==3:\n",
    "        return inve_sol(x5)*10-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb24ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gate\n",
    "def gate(x5,label):\n",
    "    return np.sum(x5,label)*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1ca80",
   "metadata": {},
   "source": [
    "### After decoder and gate, four XGBoost models are adopted for specific tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50496595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class second_stage_multi_XGB(nn.Module):\n",
    "    def init(self, input_dim_clf, output_dim_clf,input_dim_reg,\n",
    "             output_dim_reg, learning_rate, num_epochs):\n",
    "        super(second_stage_multi_XGB, self).init()\n",
    "        self.input_dim_clf = input_dim_clf\n",
    "        self.output_dim_clf = output_dim_clf\n",
    "        self.input_dim_reg = input_dim_reg\n",
    "        self.output_dim_reg = output_dim_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.linear = nn.Linear(self.input_dim_cls, self.output_dim_cls)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion_cls = nn.MSELoss()\n",
    "        self.criterion_reg = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.xgb_clf = xgb.XGBClassifier()\n",
    "        self.xgb_reg1 = xgb.XGBRegressor()\n",
    "        self.xgb_reg2 = xgb.XGBRegressor()\n",
    "        self.xgb_reg3 = xgb.XGBRegressor()        \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Classification task\n",
    "        clf_out = self.xgb_clf(x)\n",
    "        # Regression tasks\n",
    "        reg1_out = self.xgb_reg1(x)\n",
    "        reg2_out = self.xgb_reg2(x)\n",
    "        reg3_out = self.xgb_reg3(x)\n",
    "        return clf_out, reg1_out, reg2_out, reg3_out\n",
    "    \n",
    "    def multi_task_loss(clf_out,reg1_out, reg2_out, reg3_out,clf_target,reg1_target, reg2_target, reg3_target):\n",
    "        clf_loss = F.cross_entropy(clf_out, clf_target)\n",
    "        reg1_loss = F.mse_loss(reg1_out, reg1_target)\n",
    "        reg2_loss = F.mse_loss(reg2_out, reg2_target)\n",
    "        reg3_loss = F.mse_loss(reg3_out, reg3_target)\n",
    "        return clf_loss + reg1_loss + reg2_loss + reg3_loss\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        for epoch in range(self.num_epochs):            \n",
    "            # Forward pass\n",
    "            outputs = self.forward(x)\n",
    "            # Calculate loss\n",
    "            loss = loss = multi_task_loss(clf_out, reg1_out, reg2_out, reg3_out, clf_target, reg1_target, reg2_target, reg3_target)\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # Print loss\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, self.num_epochs, loss.item()))\n",
    "\n",
    "MT_XGB = second_stage_multi_XGB(input_dim_cls=\"Number of logging data types\", output_dim_cls=4,input_dim_reg=\"Number of logging data types\",\n",
    "             output_dim_reg=1, learning_rate=\"The_learning_rate_you_want\", num_epochs=\"The_num_epochs_you_want\")\n",
    "MT_XGB.train(\"Logging data\", \"Label\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
